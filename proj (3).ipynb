{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liYFtdnDjdE1",
        "outputId": "b4a33e46-92d6-429c-90f0-f151c9a2660e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=901116d53afed515d727fa01c99ee1c3a80c576b47ef560edb65db170e82743f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "pip install langdetect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr-V7oD9lYev",
        "outputId": "27483285-5b53-426d-83ad-899e0840571c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from langid) (1.26.4)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941171 sha256=8309af83ba9e95422f984b275d57f27c11dcfb133b1dd71cf5d9d9e45bb21059\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/6a/b6/b7eb43a6ad55b139c15c5daa29f3707659cfa6944d3c696f5b\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ],
      "source": [
        "pip install langid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnlc7l9SkbVk",
        "outputId": "2418649b-aaed-4d21-e0a9-2f9eb933d535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3srZpBvxaT2",
        "outputId": "a9d890ab-721d-4380-c9e6-6643010371a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzODvRbkzC7D"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVVVygbu0FEJ",
        "outputId": "e9c0cdca-5e5c-4318-f8b8-e7420f59fe73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.11/dist-packages (from hmmlearn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\n",
            "Downloading hmmlearn-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.9/165.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install hmmlearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoujDMrP56-J"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"The default value of `n_init` will change from 10 to 'auto' in 1.4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR7cq6P-AQ-2",
        "outputId": "3c488fdb-d28b-403c-e366-9cb57827584a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.base:Model is not converging.  Current: 16598665.449641122 is not greater than 16598665.449641204. Delta is -8.195638656616211e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8793859649122807\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import sys\n",
        "import warnings\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Suppress warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Load the dataset and select only the first 3000 rows\n",
        "languages = pd.read_csv('/content/Language Detection.csv').head(2280)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(languages, test_size=0.2)\n",
        "\n",
        "# Function to extract N-gram features\n",
        "def extract_ngrams(text, n):\n",
        "    tokens = word_tokenize(text)\n",
        "    return [' '.join(gram) for gram in ngrams(tokens, n)]\n",
        "\n",
        "# Extract N-gram features for the training data\n",
        "X_train = train_data['Text']\n",
        "y_train = train_data['Language']\n",
        "\n",
        "# Use CountVectorizer to convert text data into a matrix of token counts\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_count = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_count, y_train)\n",
        "\n",
        "# Train the Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(X_train_count, y_train)\n",
        "\n",
        "# Train the HMM model for each language\n",
        "hmm_models = {}\n",
        "for language in languages['Language'].unique():\n",
        "    language_data = languages[languages['Language'] == language]\n",
        "    hmm_model = hmm.GaussianHMM(n_components=2)  # Use GaussianHMM for faster training\n",
        "    hmm_model.fit(vectorizer.transform(language_data['Text']).toarray())\n",
        "    hmm_models[language] = hmm_model\n",
        "\n",
        "# Function to predict languages using HMM and Naive Bayes\n",
        "def predict_languages(text):\n",
        "    text_count = vectorizer.transform([text])\n",
        "    nb_prediction = nb_classifier.predict(text_count)\n",
        "    hmm_predictions = {}\n",
        "    for language, hmm_model in hmm_models.items():\n",
        "        hmm_predictions[language] = hmm_model.score(text_count.toarray())\n",
        "    return nb_prediction, hmm_predictions\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "test_texts = test_data['Text']\n",
        "test_languages = test_data['Language']\n",
        "predicted_languages = []\n",
        "for text in test_texts:\n",
        "    nb_prediction, hmm_predictions = predict_languages(text)\n",
        "    predicted_language = max(hmm_predictions, key=hmm_predictions.get)\n",
        "    predicted_languages.append(predicted_language)\n",
        "\n",
        "accuracy = accuracy_score(test_languages, predicted_languages)\n",
        "print('Accuracy:', accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "w1MwK_lXRO7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.data.path.append('/root/nltk_data')\n"
      ],
      "metadata": {
        "id": "E7IhBm3demBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y nltk\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "t0yqEVnde1WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.data\n",
        "print(nltk.data.find('tokenizers/punkt'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH5iAPtZfVu1",
        "outputId": "9ea245af-07ca-4445-cc88-774586ec5165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/nltk_data/tokenizers/punkt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.data.path.append('/root/nltk_data')\n"
      ],
      "metadata": {
        "id": "dz-bR6WAfb6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')\n"
      ],
      "metadata": {
        "id": "kxRagj9Yfd3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L16hCaIWaekA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "def predict_sentence_languages(text, hmm_threshold=-1000):\n",
        "    words = word_tokenize(text)\n",
        "    detected_languages = set()\n",
        "\n",
        "    for word in words:\n",
        "        # Ensure the word exists in the vectorizer vocabulary\n",
        "        if word not in vectorizer.get_feature_names_out():\n",
        "            continue  # Skip unseen words\n",
        "\n",
        "        word_count = vectorizer.transform([word])\n",
        "        nb_prediction = nb_classifier.predict(word_count)[0]\n",
        "        hmm_predictions = {}\n",
        "\n",
        "        # Get HMM predictions\n",
        "        for language, hmm_model in hmm_models.items():\n",
        "            try:\n",
        "                score = hmm_model.score(np.array(word_count.toarray()))  # Ensure proper input format\n",
        "                if score > hmm_threshold:\n",
        "                    hmm_predictions[language] = score\n",
        "            except:\n",
        "                continue  # Skip if the HMM model fails\n",
        "\n",
        "        detected_languages.add(nb_prediction)\n",
        "\n",
        "        if hmm_predictions:\n",
        "            hmm_prediction = max(hmm_predictions, key=hmm_predictions.get)\n",
        "            detected_languages.add(hmm_prediction)\n",
        "\n",
        "    return list(detected_languages)\n",
        "\n",
        "# Example input\n",
        "text = input(\"Enter a sentence: \")\n",
        "predicted_languages = predict_sentence_languages(text)\n",
        "\n",
        "print(\"Detected Languages:\", predicted_languages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "malyalam, tamil, kannada, hindi, kokborok, english, punjabi."
      ],
      "metadata": {
        "id": "lZsT1Cp1bKK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_word_languages(text, hmm_threshold=-1000):  # Set a reasonable score threshold\n",
        "    words = word_tokenize(text)\n",
        "    word_languages = {}  # Dictionary to store each word and its predicted language\n",
        "\n",
        "    for word in words:\n",
        "        word_count = vectorizer.transform([word])\n",
        "        nb_prediction = nb_classifier.predict(word_count)[0]\n",
        "        hmm_predictions = {}\n",
        "\n",
        "        # Get HMM predictions for the word\n",
        "        for language, hmm_model in hmm_models.items():\n",
        "            score = hmm_model.score(word_count.toarray())\n",
        "            if score > hmm_threshold:  # Only consider languages with scores above the threshold\n",
        "                hmm_predictions[language] = score\n",
        "\n",
        "        # Choose the highest scoring HMM prediction if any\n",
        "        if hmm_predictions:\n",
        "            hmm_prediction = max(hmm_predictions, key=hmm_predictions.get)\n",
        "        else:\n",
        "            hmm_prediction = nb_prediction  # Fallback to Naive Bayes if no HMM prediction\n",
        "\n",
        "        # Store the word and its predicted language\n",
        "        word_languages[word] = hmm_prediction\n",
        "\n",
        "    # Return the dictionary of words and their detected languages\n",
        "    return word_languages\n",
        "\n",
        "# Example input text\n",
        "text = input()\n",
        "predicted_word_languages = predict_word_languages(text)\n",
        "\n",
        "# Print the detected language for each word in the sentence\n",
        "for word, language in predicted_word_languages.items():\n",
        "    print(f\"{word} - {language}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-b7tBB1NZCS",
        "outputId": "77acd1ad-6a7c-479a-f7f7-eb817a7931b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unkal peyar enna what is your name aapka name kya hai\n",
            "Unkal - Malayalam\n",
            "peyar - Malayalam\n",
            "enna - Malayalam\n",
            "what - English\n",
            "is - English\n",
            "your - English\n",
            "name - English\n",
            "aapka - Malayalam\n",
            "kya - Malayalam\n",
            "hai - Malayalam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.data\n",
        "print(nltk.data.find('tokenizers/punkt'))\n"
      ],
      "metadata": {
        "id": "fFzUTy31SLTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure necessary downloads\n",
        "nltk.download('punkt')\n",
        "\n",
        "def predict_word_languages(text, hmm_threshold=25000, score_margin=2000):\n",
        "    words = word_tokenize(text)\n",
        "    word_languages = {}\n",
        "\n",
        "    for word in words:\n",
        "        if word in [\",\", \".\", \"!\", \"?\", \"'\", '\"']:\n",
        "            word_languages[word] = 'English'\n",
        "            continue\n",
        "\n",
        "        word_count = vectorizer.transform([word])  # Ensure vectorizer is defined\n",
        "        nb_prediction = nb_classifier.predict(word_count)[0]  # Ensure classifier is defined\n",
        "        hmm_predictions = {}\n",
        "\n",
        "        for language, hmm_model in hmm_models.items():  # Ensure hmm_models is defined\n",
        "            score = hmm_model.score(word_count.toarray())\n",
        "            print(f\"Word: {word}, HMM {language} score: {score}\")  # Debugging output\n",
        "            if score > hmm_threshold:\n",
        "                hmm_predictions[language] = score\n",
        "\n",
        "        if hmm_predictions:\n",
        "            hmm_prediction = max(hmm_predictions, key=hmm_predictions.get)\n",
        "            hmm_max_score = hmm_predictions[hmm_prediction]\n",
        "\n",
        "            if hmm_max_score - hmm_predictions.get(nb_prediction, 0) < score_margin:\n",
        "                final_prediction = nb_prediction\n",
        "            else:\n",
        "                final_prediction = hmm_prediction\n",
        "        else:\n",
        "            final_prediction = nb_prediction\n",
        "\n",
        "        word_languages[word] = final_prediction\n",
        "        print(f\"Word: {word}, Naive Bayes prediction: {nb_prediction}, HMM prediction: {final_prediction}\")\n",
        "\n",
        "    return word_languages\n",
        "\n",
        "# Example input text\n",
        "text = input(\"Enter a sentence: \")\n",
        "predicted_word_languages = predict_word_languages(text)\n",
        "\n",
        "for word, language in predicted_word_languages.items():\n",
        "    print(f\"{word} - {language}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xLfEdv6N5sY",
        "outputId": "cccf1b78-5a75-4437-e166-12062637554b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: hello aap kese ho\n",
            "Word: hello, HMM English score: 23506.48267031727\n",
            "Word: hello, HMM Malayalam score: 1318.1061427863424\n",
            "Word: hello, HMM Hindi score: 10669.055559417313\n",
            "Word: hello, HMM Tamil score: 15614.105295650636\n",
            "Word: hello, Naive Bayes prediction: English, HMM prediction: English\n",
            "Word: aap, HMM English score: 23679.403166640797\n",
            "Word: aap, HMM Malayalam score: 28868.10614278634\n",
            "Word: aap, HMM Hindi score: 10819.055559417313\n",
            "Word: aap, HMM Tamil score: 26814.105295650636\n",
            "Word: aap, Naive Bayes prediction: English, HMM prediction: Malayalam\n",
            "Word: kese, HMM English score: 23679.403166640797\n",
            "Word: kese, HMM Malayalam score: 28868.10614278634\n",
            "Word: kese, HMM Hindi score: 10819.055559417313\n",
            "Word: kese, HMM Tamil score: 26814.105295650636\n",
            "Word: kese, Naive Bayes prediction: English, HMM prediction: Malayalam\n",
            "Word: ho, HMM English score: 23679.403166640797\n",
            "Word: ho, HMM Malayalam score: 28868.10614278634\n",
            "Word: ho, HMM Hindi score: 10819.055559417313\n",
            "Word: ho, HMM Tamil score: 26814.105295650636\n",
            "Word: ho, Naive Bayes prediction: English, HMM prediction: Malayalam\n",
            "hello - English\n",
            "aap - Malayalam\n",
            "kese - Malayalam\n",
            "ho - Malayalam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Predict languages using Naive Bayes for the test data\n",
        "nb_predicted_languages = nb_classifier.predict(vectorizer.transform(test_texts))\n",
        "\n",
        "# Calculate metrics for Naive Bayes\n",
        "nb_accuracy = accuracy_score(test_languages, nb_predicted_languages)\n",
        "nb_precision = precision_score(test_languages, nb_predicted_languages, average='weighted')\n",
        "nb_recall = recall_score(test_languages, nb_predicted_languages, average='weighted')\n",
        "nb_f1 = f1_score(test_languages, nb_predicted_languages, average='weighted')\n",
        "\n",
        "print(\"Naive Bayes Classifier Metrics:\")\n",
        "print(f\"Accuracy: {nb_accuracy}\")\n",
        "print(f\"Precision: {nb_precision}\")\n",
        "print(f\"Recall: {nb_recall}\")\n",
        "print(f\"F1 Score: {nb_f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeLu_wgUKT8e",
        "outputId": "03eeb11f-8d8c-458e-da76-6f09fa320b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Classifier Metrics:\n",
            "Accuracy: 0.9912280701754386\n",
            "Precision: 0.9912439039837863\n",
            "Recall: 0.9912280701754386\n",
            "F1 Score: 0.9911962085484335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict languages using Decision Tree for the test data\n",
        "dt_predicted_languages = dt_classifier.predict(vectorizer.transform(test_texts))\n",
        "\n",
        "# Calculate metrics for Decision Tree\n",
        "dt_accuracy = accuracy_score(test_languages, dt_predicted_languages)\n",
        "dt_precision = precision_score(test_languages, dt_predicted_languages, average='weighted')\n",
        "dt_recall = recall_score(test_languages, dt_predicted_languages, average='weighted')\n",
        "dt_f1 = f1_score(test_languages, dt_predicted_languages, average='weighted')\n",
        "\n",
        "print(\"\\nDecision Tree Classifier Metrics:\")\n",
        "print(f\"Accuracy: {dt_accuracy}\")\n",
        "print(f\"Precision: {dt_precision}\")\n",
        "print(f\"Recall: {dt_recall}\")\n",
        "print(f\"F1 Score: {dt_f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3n4FOk_KXb4",
        "outputId": "623734a8-2922-44b3-b74c-46ca709ee9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree Classifier Metrics:\n",
            "Accuracy: 0.9649122807017544\n",
            "Precision: 0.9698628510902517\n",
            "Recall: 0.9649122807017544\n",
            "F1 Score: 0.9651129801614411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize list to store HMM predictions\n",
        "hmm_predicted_languages = []\n",
        "\n",
        "# Loop through test texts to predict using HMM\n",
        "for text in test_texts:\n",
        "    _, hmm_predictions = predict_languages(text)\n",
        "    predicted_language = max(hmm_predictions, key=hmm_predictions.get)  # The language with the highest score\n",
        "    hmm_predicted_languages.append(predicted_language)\n",
        "\n",
        "# Calculate metrics for HMM\n",
        "hmm_accuracy = accuracy_score(test_languages, hmm_predicted_languages)\n",
        "hmm_precision = precision_score(test_languages, hmm_predicted_languages, average='weighted')\n",
        "hmm_recall = recall_score(test_languages, hmm_predicted_languages, average='weighted')\n",
        "hmm_f1 = f1_score(test_languages, hmm_predicted_languages, average='weighted')\n",
        "\n",
        "print(\"\\nHidden Markov Model Metrics:\")\n",
        "print(f\"Accuracy: {hmm_accuracy}\")\n",
        "print(f\"Precision: {hmm_precision}\")\n",
        "print(f\"Recall: {hmm_recall}\")\n",
        "print(f\"F1 Score: {hmm_f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzuzbHMBKZE-",
        "outputId": "77750145-17a8-4e4e-8ec1-7674f4d0bce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hidden Markov Model Metrics:\n",
            "Accuracy: 0.8925438596491229\n",
            "Precision: 0.9475015463890843\n",
            "Recall: 0.8925438596491229\n",
            "F1 Score: 0.9136341899717371\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}